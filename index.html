<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jiali Chen</title>
  
  <meta name="author" content="Jiali Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar.jpeg">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Jiali Chen &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <img style="vertical-align:middle" src='images/gary_chen_name.png' height='38px' width='WIDTHpx'>
		  </name>
        </p>
		<p>Currently, Jiali Chen is the first-year direct Ph.D. student at <a href="https://www2.scut.edu.cn/klbdir/">Key Laboratory of Big Data and Intelligent Robot</a> of <a href="https://www.scut.edu.cn/new/">South China University of Technology (SCUT)</a>, supervised by <a href="https://scholar.google.com.hk/citations?user=ej3Nb5wAAAAJ&hl=zh-CN">Prof. Yi Cai</a>. He works closely with <a href="https://qianrusun.com/">Dr. Jiayuan Xie</a> at Hong Kong Polytechnic University (PolyU).
            Before that, He also obtained the B.E. degree in Department of Software Engineering from South China University of Technology (SCUT) in 2023. His research interests revolve around Multimodal Learning, Causal Inference and Text Generation.
        </p>
            Feel free to contact me if you are interested in my work or seeking potential collaborations.
            <p align=center>
          <a href="mailto:segarychen[at]mail.scut.edu.cn">Email</a> &nbsp/&nbsp
<!--          <a href="image/cv_gary.pdf">CV</a> &nbsp/&nbsp-->
		  <a href="https://scholar.google.com.hk/citations?user=gO4aiQoAAAAJ&hl=zh-CN">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/Gary-code">Github</a>

        </p>

        </td>
        <td width="33%">
        <img src="images/jiali.jpg" width="128">
        </td>
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>News</heading>
          <p>
<!--              TODO-->
<!--          <li> <strongsmall>[2024/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Dr. Jiayuan Xie is accepted by TIP 2024.</smalll><br/>-->
          <li> <strongsmall>[2023/08]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ACM MM 2023 !</smalll><br/>
          <li> <strongsmall>[2023/01]</strongsmall> &nbsp;&nbsp;<smalll>1 paper with Dr. Jiayuan Xie is accepted by TOMM 2023.</smalll><br/>
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='images/scut.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>South China University of Technology (SCUT), China</stronghuge><br />
          Honours Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2019 - Jun. 2023 <br />
          Excellent Degree Dissertations of SCUT in 2023.
          </p>
        </td>
      </tr>


        <tr>
          <td width="10%">
            <img src='images/scut.png' width="100">
          </td>

          <td width="90%" valign="middle">
          <p>
          <stronghuge>South China University of Technology (SCUT), China</stronghuge><br />
          <a href="https://www2.scut.edu.cn/klbdir/">Key Laboratory of Big Data and Intelligent Robot</a> &nbsp;&nbsp; &bull;  Sep. 2023 - Present <br/>
          Supervisor: <a href="https://scholar.google.com.hk/citations?user=ej3Nb5wAAAAJ&hl=zh-CN">Prof. Yi Cai</a>
          
          </p>
        </td>
      </tr>
	  
	  
<!--	    <tr>-->
<!--          <td width="10%">-->
<!--            <img src='image/ntu_icon.jpg' width="100">-->
<!--          </td>-->

<!--          <td width="75%" valign="middle">-->
<!--          <p>-->
<!--          <stronghuge>Nanyang Technological University (NTU), Singapore</stronghuge><br />-->
<!--          Second-year Ph.D. in <a href="https://mreallab.github.io/">MreaL Lab</a>, School of Computer Science and Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Aug. 2020 - Present <br />-->
<!--          Supervisor: Prof. <a href="https://personal.ntu.edu.sg/hanwangzhang/">Zhang Hanwang</a>-->
<!--          </p>-->
<!--        </td>-->
<!--      </tr>-->
	  
      </table>





<!--<p></p><p></p><p></p><p></p><p></p>-->

<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Research Experience</heading>-->
<!--        </td>-->
<!--      </tr>-->
<!--      </table>-->

<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->
<!--          <td width="10%">-->
<!--            <a href="http://cfm.uestc.edu.cn/">-->
<!--            <img src='image/cfm_icon4.png' width="100">-->
<!--            </a>-->
<!--          </td>-->

<!--          <td width="80%" valign="middle">-->
<!--          <p>-->
<!--          <stronghuge>Center For Future Media, UESTC</stronghuge><br />-->
<!--          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Mar. 2018 - Jun. 2020 <br />-->
<!--          Advisors: &nbsp; Prof. <a href="https://interxuxing.github.io/">Xing Xu</a> and Prof. <a href="http://cfm.uestc.edu.cn/~yangyang/">Yang Yang</a>.  &nbsp;&nbsp;Collaborated with  Prof. <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/intelligent-systems/multimedia-computing/people/alan-hanjalic/">Alan Hanjalic</a><br/>&lt;!&ndash;-->
<!--          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>-->
<!--          <li> Combined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>-->
<!--          <li> Complete 3 works and make the submission.&ndash;&gt;-->
<!--          </p>-->
<!--        </td>-->
<!--      </tr>-->


<!--      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->
<!--          <td width="10%">-->
<!--            <a href="https://mreallab.github.io/people.html">-->
<!--            <img src='image/mreal_icon.png' width="100">-->
<!--          </a>-->
<!--          </td>-->

<!--          <td width="80%" valign="middle">-->
<!--          <p>-->
<!--          <stronghuge>MReal Lab, NTU</stronghuge><br />-->
<!--          <huge><em>Research  Assistant</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; July. 2019 - Aug. 2020 <br />-->
<!--          Advisors: &nbsp; Prof. <a href="https://personal.ntu.edu.sg/hanwangzhang/">Hanwang Zhang </a>&lt;!&ndash;-->
<!--          <li> Proposed several novel methods for cross-modal retrieval which achieves the state-of-the-art performance on image-text matching.<br/>-->
<!--          <li> Comnined the GCN with Visual Question Generation Task and further boost the performance on an unexplored challenging task zero-shot VQA. <br/>-->
<!--          <li> Complete 3 works and make the submission.&ndash;&gt;-->
<!--          </p>-->
<!--        </td>-->
<!--      </tr>-->
<!--	  -->
<!--	  -->
<!--	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->
<!--          <td width="10%">-->
<!--            <a href="https://www.microsoft.com/en-us/research/">-->
<!--            <img src='image/microsoft_icon3.jpg' width="100">-->
<!--          </a>-->
<!--          </td>-->

<!--          <td width="80%" valign="middle">-->
<!--          <p>-->
<!--          <stronghuge>Micorsoft Research, Redmond WA (Remote)</stronghuge><br />-->
<!--          <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Jun. 2022 - Sep. 2022 <br />-->
<!--          Advisors: &nbsp; <a href="https://sites.google.com/site/kevinlin311tw/me">Kevin Lin</a>, <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Lindsey Li</a>-->
<!--          </p>-->
<!--        </td>-->
<!--      </tr>-->
<!--          -->
<!--  -->
<!--	  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--        <tr>-->
<!--          <td width="12%">-->
<!--            <a href="https://research.google/">-->
<!--            <img src='image/google_icon2.png' width="100">-->
<!--          </a>-->
<!--          </td>-->

<!--          <td width="80%" valign="middle">-->
<!--          <p>-->
<!--          <stronghuge>Google Research</stronghuge><br />-->
<!--          <huge><em>Research Intern</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; June. 2023 - Oct. 2023 <br />-->
<!--          </p>-->
<!--        </td>-->
<!--      </tr>-->

<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication <a href="https://scholar.google.com.hk/citations?user=gO4aiQoAAAAJ&hl=zh-CN" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>

<!--TIP 2024 KICNLE-->
<!--          TODO-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--      <td align="center" width="25%">-->
<!--        <img src='projects/KICNLE/model.png'  width="200" height="100">-->
<!--      </td>-->
<!--      <td valign="top" width="75%">-->
<!--	 <strong>Knowledge-Augmented Visual Question Answering with Natural Language Explanation-->
<!--</strong><br>-->
<!--          <em>IEEE Transactions on Image Processing, <strong>TIP 2024</strong> </em> <br>-->
<!--          <a href="KICNLE_TODO">[Paperlink]</a>, <a href="https://github.com/Gary-code/KICNLE">[Code]</a><br>-->
<!--          <em>Area: VQA, Reasoning, Text Generation</em> <br>-->
<!--		<p> We introduce KICNLE, which generates consistent answer and explanation with external knowledge. </p>-->
<!--      </td>-->
<!--    </tr>-->
<!--</table>-->

<!--TOMM 2024 Diverse VQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td align="center" width="25%">
            <img src='projects/DiverseVQG/model.png'  width="200" height="100">
        </td>
        <td valign="top" width="75%">
            <strong>Diverse Visual Question Generation based on Multiple Objects Selection
            </strong><br>
            Wenhao Fang, Jiayuan Xie, Hongfei Liu, <strong>Jiali Chen</strong>, Yi Cai<br>
            <em>Transactions on Multimedia Computing Communications and Applications, <strong>TOMM 2024</strong></em><br>
            <!--<em>(Internship Project at Microsoft)</em> <br>-->
            <a href="https://dl.acm.org/doi/abs/10.1145/3640014">[Paperlink]</a><br>
            <em>Area: Diverse Text Generation, Visual Question Generation</em> <br>
            <p></p>
            <p>We promote the semantic diversity of generated questions beyond the description diversity.</p>
        </td>
    </tr>
</table>


<!--    MM 2023 KEVQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td align="center" width="25%">
        <img src='projects/KECVQG/model.png'  width="200" height="120">
      </td>
      <td valign="top" width="75%">
	 <strong>Deconfounded Visual Question Generation with Causal Inference
</strong><br>
	 <strong>Jiali Chen</strong>, Zhenjun Guo, Jiayuan Xie, Yi Cai, Qing Li<br>
          <em>ACM Multimedia, <strong>ACM MM 2023</strong></em><br>
		<!--<em>(Internship Project at Microsoft)</em> <br>-->
		<a href="https://dl.acm.org/doi/10.1145/3581783.3612536">[Paperlink]</a>, <a href="https://github.com/Gary-code/KECVQG">[Code]</a><br>
        <em>Area: Bias, Causal Inference, Visual Question Generation</em> <br>
        <p></p>
		<p>This study first introduces a causal perspective on VQG and adopts the causal graph to analyze spurious correlations among variables. We propose KECVQG mitigates the impact of spurious correlations for VQG.</p>
      </td>
    </tr>
</table>
          
          

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='projects/NN-TarVQG/model.png'  width="200" height="120">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Question Generation for Explicit Questioning Purposes based on Target Objects</strong><br>
	 Jiayuan Xie*, <strong>Jiali Chen*</strong>, Wenhao Fang, Yi Cai, Qing Li<br>
    <em>Neural Network, <strong>NN 2023</strong></em><br>
		<a href="https://www.sciencedirect.com/science/article/pii/S0893608023004264">[Paperlink]</a>, <a href="https://github.com/Gary-code/MOAG">[Code]</a><br>
        <em>Area: Controllable Text Generation, Visual Question Generation</em> <br>
        <p></p>
		<p>We propose a content-controlled question generation model, which generates questions based on a given target object set specified from an image.</p>
      </td>
    </tr>
</table>
   
<!--   TOMM 2023 Ob-VQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
        <img src='projects/Ob-VPG/model.png'  width="200" height="150">
      </td>
      <td valign="top" width="75%">
	 <strong>Visual Paraphrase Generation with Key Information Retained</strong><br>
          Jiayuan Xie, <strong>Jiali Chen</strong>, Yi Cai, Qingbao Huang, Qing Li <br>
        <em>ACM Transactions on Multimedia Computing, Communications and Applications, <strong>TOMM 2023</strong></em><br>
		
		<a href="https://dl.acm.org/doi/full/10.1145/3585010">[Paperlink]</a>, <a href="https://github.com/Gary-code/Ob-VPG">[Code]</a><br>
        <em>Area: Vision-Language, Controllable Text Generation</em> <br>
        <p></p>
		<p>We propose an object-level paraphrase generation model, which generates paraphrases by adjusting the permutation of key objects and modifying their associated descriptions.</p>
      </td>
    </tr>
   </table>

<!--        AAAI 2023 SA Cate-VQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
            <img src='projects/Cate-VQG/model.png'  width="200" height="80">
        </td>
        <td valign="top" width="75%">
            <strong>Visual Paraphrase Generation with Key Information Retained</strong><br>
            Hongfei Liu, <strong>Jiali Chen</strong>, Wenhao Fang, Jiayuan Xie, Yi Cai <br>
            <em>Proceedings of the AAAI Conference on Artificial Intelligence, <strong>AAAI Student Abstract 2023</strong></em><br>

            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26991">[Paperlink]</a><br>
            <em>Area: Controllable Text Generation, Visual Question Generation</em> <br>
            <p></p>
            <p>A category-guided visual question generation model that can generate questions with multiple categories that focus on different objects in an image.</p>
        </td>
    </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Academic Service</heading>
          <div style="line-height:25px">
          <p>
<!--		  <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/>-->
		  <li> <stronghuge>Conference Reviewer:</stronghuge> &nbsp; NAACL, EMNLP, KDD, WSDM, ACM MM<br/>
<!--		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TNNLS, ACM ToMM<br/>-->
          </p>
          </div>
        </td>
      </tr>
</table>




<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Talk</heading>-->
<!--          <div style="line-height:25px">-->
<!--          <p>-->
<!--          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", Microsoft, Reading Group, 2023.06<br/>-->
<!--          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", KAUST, Rising Stars in AI Symposium, 2023.03<br/>-->
<!--          <li> "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", JiangMen Talk, 2022.10<br/>-->
<!--		  <li> "Self-Supervised Learning Disentangled Group Representation as Feature", PREMIA AGM 2022, 2022.08<br/>-->
<!--		  <li> "Towards Out-of-Distribution Generalization in Computer Vision", National University of Singapore (NUS), 2022.04<br/>-->
<!--		  <li> "Disentangled Group Representation Learning and its Potential in Causality", ZhiYuan Community, 2022.01<br/>-->
<!--		  <li> "Generalization Powered by Invariant Learning", Singapore Management University (SMU), 2021.11<br/>-->
<!--		  <li> "因果推理的应用与发展 (中文)", AI Time, 2021.10 &nbsp<a href="https://www.bilibili.com/video/BV1qP4y1b7us?spm_id_from=333.999.0.0">[Video]</a><br/>-->
<!--		  <li> "Visual Commonsense R-CNN", National University of Singapore (NUS), 2021.03<br/>-->
<!--          </p>-->
<!--          </div>-->
<!--        </td>-->
<!--      </tr>-->
<!--</table>-->



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
		  <li> <stronghuge>Excellent Degree Dissertations of SCUT</stronghuge>,&nbsp; 2023<br/>
		  <li> <stronghuge>Global AI Challenge for Building E&M Facilities Golden Award</stronghuge>,&nbsp; 2023<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<!--<p></p><p></p><p></p><p></p><p></p>-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Project</heading>-->
<!--		  <p>-->
<!--		  <div style="text-align: center;">-->
<!--		  <img src='project/projects_icon.png'  width="600">-->
<!--		  </p>-->
<!--          </div>-->
<!--        </td>-->
<!--      </tr>-->
<!--</table>-->



<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Leadership Experience</heading>-->
<!--      </td>-->
<!--      </tr>-->
<!--      </table>-->


<!--    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--      <td width="15%">-->
<!--        <img src='image/lecture.jpg'  width="195" height="130">-->
<!--      </td>-->
<!--      <td valign="top" width="75%">-->
<!--        <stronghuge>Lecture Group of EE Department</stronghuge> </br>-->
<!--        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>-->
<!--        <p></p>-->
<!--        <p>-->
<!--         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>-->
<!--          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>-->
<!--          </p> -->
<!--      </td>-->
<!--    </tr>-->
<!--   </table>-->


<!--       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >-->
<!--      <td width="26%">-->
<!--        <div class="one">-->
<!--                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>-->
<!--                <img src='image/dachuang1.png'  width="195" height="130">-->
<!--              </div>-->
<!--      <script type="text/javascript">-->
<!--                function dachuang_start() {-->
<!--                  document.getElementById('dachuang_image').style.opacity = "1";-->
<!--                }-->
<!--                function dachuang_stop() {-->
<!--                  document.getElementById('dachuang_image').style.opacity = "0";-->
<!--                }-->
<!--                dachuang_stop()-->
<!--              </script>-->
<!--      </td>-->

<!--      <td valign="top" width="75%">-->
<!--        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>-->
<!--        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>-->
<!--        <p></p>-->
<!--        <p>-->
<!--         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>-->
<!--         <li> Responsible for the code implementation and project promotion.<br/>-->
<!--          </p> -->
<!--      </td>-->
<!--    </tr>-->
<!--   </table>-->
          




<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Personal Interests</heading>-->
<!--          <p>-->
<!--          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)-->
<!--          </p>-->
<!--          <p>-->
<!--          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.-->
<!--          </p>-->
<!--      </td>-->
<!--      </tr>-->





<!--   <p></p><p></p><p></p><p></p><p></p>-->
<!--   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>-->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on Jan, 2024
				<p align="middle"><font size="2">
				This awesome template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">[here]</a>
				</tbody></table>
   

</body>
</html>
