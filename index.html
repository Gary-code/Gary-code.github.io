<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jiali Chen</title>

  <meta name="author" content="Jiali Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar.jpeg">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Jiali Chen &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  <img style="vertical-align:middle" src='images/gary_chen_name.png' height='38px' width='WIDTHpx'>
		  </name>
        </p>
		<p>Currently, Jiali Chen is the second-year direct Ph.D. student at <a href="https://www2.scut.edu.cn/klbdir/">Key Laboratory of Big Data and Intelligent Robot</a> of <a href="https://www.scut.edu.cn/new/">South China University of Technology (SCUT)</a>, supervised by <a href="https://scholar.google.com.hk/citations?user=ej3Nb5wAAAAJ&hl=zh-CN">Prof. Yi Cai</a>. He works closely with <a href="https://scholar.google.com/citations?hl=zh-CN&user=yZOXh24AAAAJ&view_op=list_works&sortby=pubdate">Dr. Jiayuan Xie</a> at Hong Kong Polytechnic University (PolyU).
            Before that, He also obtained the B.E. degree in Department of Software Engineering from South China University of Technology (SCUT) in 2023. His research interests revolve around Multimodal Reasoning, Causal Inference and Vision & Language.
        </p>
            Feel free to contact me if you're interested in discussing or seeking potential collaborations.
            <p align=center>
          <a href="mailto:segarychen[at]mail.scut.edu.cn">Email</a> &nbsp/&nbsp
<!--          <a href="image/cv_gary.pdf">CV</a> &nbsp/&nbsp-->
		  <a href="https://scholar.google.com.hk/citations?user=gO4aiQoAAAAJ&hl=zh-CN">Google Scholar</a>  &nbsp/&nbsp
          <a href="https://github.com/Gary-code">Github</a>

        </p>

        </td>
          <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiali_2.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/jiali_2.jpg" class="hoverZoomLink"></a>
          </td>
<!--        <td width="33%">-->
<!--        <img src="images/jiali_1.jpg" width="128">-->
<!--        </td>-->
      </tr>
      </table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>üî• News</heading>
          <p>
<!--              TODO-->
          <li> <strongsmall>[2025/01]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by NAACL 2025!</smalll><br/>
          <li> <strongsmall>[2024/07]</strongsmall> &nbsp;&nbsp;<smalll>2 papers are accepted by ACM MM 2024!</smalll><br/>
          <li> <strongsmall>[2024/03]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by TCSVT 2024.</smalll><br/>
          <li> <strongsmall>[2024/02]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by TIP 2024.</smalll><br/>
          <li> <strongsmall>[2023/08]</strongsmall> &nbsp;&nbsp;<smalll>1 paper is accepted by ACM MM 2023!</smalll><br/>
          </p>
        </td>
      </tr>
</table>




      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>üè´ Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='images/scut.png' width="100">
          </td>

          <td width="75%" valign="middle">
          <p>
          <stronghuge>South China University of Technology (SCUT), China</stronghuge><br />
          Honours Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2019 - Jun. 2023 <br />
          Excellent Degree Dissertations of SCUT in 2023.
          </p>
        </td>
      </tr>


        <tr>
          <td width="10%">
            <img src='images/scut.png' width="100">
          </td>

          <td width="90%" valign="middle">
          <p>
          <stronghuge>South China University of Technology (SCUT), China</stronghuge><br />
          <a href="https://www2.scut.edu.cn/klbdir/">Key Laboratory of Big Data and Intelligent Robot</a> &nbsp;&nbsp; &bull;  Sep. 2023 - Present <br/>
          Supervisor: <a href="https://scholar.google.com.hk/citations?user=ej3Nb5wAAAAJ&hl=zh-CN">Prof. Yi Cai</a>

          </p>
        </td>
      </tr>


      </table>

<p></p><p></p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>üìö Selected Publication <a href="https://scholar.google.com.hk/citations?user=gO4aiQoAAAAJ&hl=zh-CN" style="font-size:22px;">[Google Scholar]</a></heading>
        </td>
      </tr>
      </table>

<!--    NAACL 2025 Classic4Children-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='./projects/Classic4Children/case.png'  width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <strong>Classic4Children: Adapting Chinese Literary Classics for Children with Large Language Model
            </strong><br>
            <strong>Jiali Chen</strong>, Xusen Hei, Yuqi Xue, Zihan Wu, Jiayuan Xie, Yi Cai<br>
            <em>Findings the Nations of the Americas Chapter of the ACL, <strong>NAACL 2025</strong></em><br>
            <!--<em>(Internship Project at Microsoft)</em> <br>-->
            <a href="https://arxiv.org/pdf/2502.01090">[Paperlink]</a>, <a href="https://github.com/Gary-code/Classic4Children/">[Code]</a><br>
            <em>Area: Large Language Model, Text Style</em> <br>
            <p></p>
            <p>We highlight children‚Äôs reading preferences: vivid character portrayals, concise narrative structure and appropriate readability are essential in adapting Chinese literary classics for children. Our proposed InstructChild
                explicitly leverages these preferences to guide the LLM in generating child-friendly text for children. Additionally, we construct the Classic4Children dataset for a comprehensive evaluation.</p>
        </td>
    </tr>
</table>

<!--    MM 2024 Learning to Correction-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='./projects/PEIFG/dfg_demo.png'  width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <strong>Learning to Correction: Explainable Feedback Generation for Visual Commonsense Reasoning Distractor
            </strong><br>
            <strong>Jiali Chen</strong>, Xusen Hei, Yuqi Xue, Yuancheng Wei, Jiayuan Xie, Yi Cai, Qing Li<br>
            <em>ACM Multimedia, <strong>ACM MM 2024</strong></em><br>
            <!--<em>(Internship Project at Microsoft)</em> <br>-->
            <a href="https://arxiv.org/abs/2412.07801">[Paperlink]</a>, <a href="https://github.com/Gary-code/PEIFG/">[Code]</a><br>
            <em>Area: Large Multimodal Model, New Benchmark</em> <br>
            <p></p>
            <p>We present the work to investigate the error correction capabilities of large multimodal models (LMMs), construct a new benchmark and introduce the feedback generation task for evaluation. <strong>I would like to extend my heartfelt gratitude to my girlfriend, Ms. Wen, for inspiring the idea behind this paper.</strong> </p>
        </td>
    </tr>
</table>

<!--    MM 2024 Unbiased Sticker-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='./projects/Emo-Sticker/case.png'  width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <strong>Deconfounded Emotion Guidance Sticker Selection with Causal Inference
            </strong><br>
            <strong>Jiali Chen</strong>, Yi Cai, Ruohang Xu, Jiexin Wang, Jiayuan Xie, Qing Li<br>
            <em>ACM Multimedia, <strong>ACM MM 2024</strong></em><br>
            <!--<em>(Internship Project at Microsoft)</em> <br>-->
            <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681522">[Paperlink]</a><br>
            <em>Area: Bias, Causal Inference, Sticker Selection</em> <br>
            <p></p>
            <p>
                This paper presents a Causal Knowledge-Enhanced Sticker Selection model that addresses spurious correlations in sticker selection by using a causal graph and a knowledge-enhanced approach.</p>
        </td>
    </tr>
</table>

<!--COLING 2024 Cross-VQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='projects/Cross-Topic-VQG/img.png'  width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <strong>Knowledge-Guided Cross-Topic Visual Question Generation
            </strong><br>
            Hongfei Liu, Guohua Wang, Jiayuan Xie, <strong>Jiali Chen</strong>, Wenhao Fang, Yi Cai<br>
            <em>International Conference on Computational Linguistics, <strong>COLING 2024</strong> </em> <br>
            <a href="https://aclanthology.org/2024.lrec-main.861/">[Paperlink]</a> <br>
            <em>Area: Knowledge, Cross-Topic, Text Generation</em> <br>
            <p> We propose a knowledge-guided cross-topic visual question generation (KC-VQG) model to extract unseen topic-related information for question generation. </p>
        </td>
    </tr>
</table>


<!--TCSVT 2024 D-VQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='projects/D-VQG/model.png' width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <strong>Video Question Generation for Dynamic Changes
            </strong><br>
            Jiayuan Xie*, <strong>Jiali Chen*</strong>, Zhenghao Liu, Qingbao Huang, Yi Cai, Qing Li<br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology, <strong>TCSVT 2024</strong> </em> <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/10505173">[Paperlink]</a>, <a href="https://github.com/Gary-code/D-VQG">[Code]</a><br>
            <em>Area: Video Understanding, Text Generation</em> <br>
            <p> We introduce D-VQG, a difference-aware video question generation model that aims to generate questions about temporal differences in the video. </p>
        </td>
    </tr>
</table>

<!--TIP 2024 KICNLE-->
<!--          TODO-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='projects/KICNLE/kb_case.png' width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
      </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
	 <strong>Knowledge-Augmented Visual Question Answering with Natural Language Explanation
</strong><br>
          <em>IEEE Transactions on Image Processing, <strong>TIP 2024</strong> </em> <br>
          <a href="https://ieeexplore.ieee.org/abstract/document/10480354/">[Paperlink]</a>, <a href="https://github.com/Gary-code/KICNLE">[Code]</a><br>
          <em>Area: VQA, Multimodal Reasoning</em> <br>
		<p> We introduce KICNLE, which generates consistent answer and explanation with external knowledge. </p>
      </td>
    </tr>
</table>

<!--TOMM 2024 Diverse VQG-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--        <td align="center" width="25%">-->
<!--            <img src='projects/DiverseVQG/model.png'  width="200" height="100">-->
<!--        </td>-->
<!--        <td valign="top" width="75%">-->
<!--            <strong>Diverse Visual Question Generation based on Multiple Objects Selection-->
<!--            </strong><br>-->
<!--            Wenhao Fang, Jiayuan Xie, Hongfei Liu, <strong>Jiali Chen</strong>, Yi Cai<br>-->
<!--            <em>Transactions on Multimedia Computing Communications and Applications, <strong>TOMM 2024</strong></em><br>-->
<!--            &lt;!&ndash;<em>(Internship Project at Microsoft)</em> <br>&ndash;&gt;-->
<!--            <a href="https://dl.acm.org/doi/abs/10.1145/3640014">[Paperlink]</a><br>-->
<!--            <em>Area: Diverse Text Generation, Visual Question Generation</em> <br>-->
<!--            <p></p>-->
<!--            <p>We promote the semantic diversity of generated questions beyond the description diversity.</p>-->
<!--        </td>-->
<!--    </tr>-->
<!--</table>-->


<!--    MM 2023 KEVQG-->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td style="padding:20px;width:20%;vertical-align:middle">
            <img src='projects/KECVQG/causal_sub.png'  width="100%" style="border-style: none; box-shadow: 5px 5px 10px rgba(0, 0, 0, 0.3); transition: filter 0.3s ease;">
      </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
	 <strong>Deconfounded Visual Question Generation with Causal Inference
</strong><br>
	 <strong>Jiali Chen</strong>, Zhenjun Guo, Jiayuan Xie, Yi Cai, Qing Li<br>
          <em>ACM Multimedia, <strong>ACM MM 2023</strong></em><br>
		<!--<em>(Internship Project at Microsoft)</em> <br>-->
		<a href="https://dl.acm.org/doi/10.1145/3581783.3612536">[Paperlink]</a>, <a href="https://github.com/Gary-code/KECVQG">[Code]</a><br>
        <em>Area: Bias, Causal Inference, Visual Question Generation</em> <br>
        <p></p>
		<p>We identify previous models frequently learn highly co-occurring object relationships and attributes, which is an inherent bias in question generation. This study first introduces a causal perspective on VQG and adopts the causal graph to analyze spurious correlations among variables. We propose KECVQG mitigates the impact of spurious correlations for VQG.</p>
      </td>
    </tr>
</table>


<!--NN 2023  VQG-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--      <td width="15%">-->
<!--        <img src='projects/NN-TarVQG/model.png'  width="200" height="120">-->
<!--      </td>-->
<!--      <td valign="top" width="75%">-->
<!--	 <strong>Visual Question Generation for Explicit Questioning Purposes based on Target Objects</strong><br>-->
<!--	 Jiayuan Xie*, <strong>Jiali Chen*</strong>, Wenhao Fang, Yi Cai, Qing Li<br>-->
<!--    <em>Neural Network, <strong>NN 2023</strong></em><br>-->
<!--		<a href="https://www.sciencedirect.com/science/article/pii/S0893608023004264">[Paperlink]</a>, <a href="https://github.com/Gary-code/MOAG">[Code]</a><br>-->
<!--        <em>Area: Controllable Text Generation, Visual Question Generation</em> <br>-->
<!--        <p></p>-->
<!--		<p>We propose a content-controlled question generation model, which generates questions based on a given target object set specified from an image.</p>-->
<!--      </td>-->
<!--    </tr>-->
<!--</table>-->

<!--   TOMM 2023 Ob-VQG-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--      <td width="15%">-->
<!--        <img src='projects/Ob-VPG/model.png'  width="200" height="150">-->
<!--      </td>-->
<!--      <td valign="top" width="75%">-->
<!--	 <strong>Visual Paraphrase Generation with Key Information Retained</strong><br>-->
<!--          Jiayuan Xie, <strong>Jiali Chen</strong>, Yi Cai, Qingbao Huang, Qing Li <br>-->
<!--        <em>ACM Transactions on Multimedia Computing, Communications and Applications, <strong>TOMM 2023</strong></em><br>-->

<!--		<a href="https://dl.acm.org/doi/full/10.1145/3585010">[Paperlink]</a>, <a href="https://github.com/Gary-code/Ob-VPG">[Code]</a><br>-->
<!--        <em>Area: Vision-Language, Controllable Text Generation</em> <br>-->
<!--        <p></p>-->
<!--		<p>We propose an object-level paraphrase generation model, which generates paraphrases by adjusting the permutation of key objects and modifying their associated descriptions.</p>-->
<!--      </td>-->
<!--    </tr>-->
<!--   </table>-->

<!--        AAAI 2023 SA Cate-VQG-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--        <td width="15%">-->
<!--            <img src='projects/Cate-VQG/model.png'  width="200" height="80">-->
<!--        </td>-->
<!--        <td valign="top" width="75%">-->
<!--            <strong>Category-Guided Visual Question Generation</strong><br>-->
<!--            Hongfei Liu, <strong>Jiali Chen</strong>, Wenhao Fang, Jiayuan Xie, Yi Cai <br>-->
<!--            <em>Proceedings of the AAAI Conference on Artificial Intelligence, <strong>AAAI Student Abstract 2023</strong></em><br>-->

<!--            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26991">[Paperlink]</a><br>-->
<!--            <em>Area: Controllable Text Generation, Visual Question Generation</em> <br>-->
<!--            <p></p>-->
<!--            <p>A category-guided visual question generation model that can generate questions with multiple categories that focus on different objects in an image.</p>-->
<!--        </td>-->
<!--    </tr>-->
<!--</table>-->



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>üìñ Academic Service</heading>
          <div style="line-height:25px">
          <p>
<!--		  <li> <stronghuge>Co-organizer:</stronghuge> &nbsp; <a href="https://nicochallenge.com/">NICO Challenge 2022</a> (ECCV'22 Workshop)<br/>-->
		  <li> <stronghuge>Conference Reviewer:</stronghuge> &nbsp; ACL, NAACL, EMNLP, ACM MM, KDD, WSDM <br/>
		  <li> <stronghuge>Journal Reviewer:</stronghuge> &nbsp; IEEE TPAMI, IEEE TIP<br/>
          </p>
          </div>
        </td>
      </tr>
</table>




<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Talk</heading>-->
<!--          <div style="line-height:25px">-->
<!--          <p>-->
<!--          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", Microsoft, Reading Group, 2023.06<br/>-->
<!--          <li> "Equivariant Similarity for Vision-Language Foundation Models (VLM)", KAUST, Rising Stars in AI Symposium, 2023.03<br/>-->
<!--          <li> "Equivariance and Invariance Inductive Bias for Learning from Insufficient Data", JiangMen Talk, 2022.10<br/>-->
<!--		  <li> "Self-Supervised Learning Disentangled Group Representation as Feature", PREMIA AGM 2022, 2022.08<br/>-->
<!--		  <li> "Towards Out-of-Distribution Generalization in Computer Vision", National University of Singapore (NUS), 2022.04<br/>-->
<!--		  <li> "Disentangled Group Representation Learning and its Potential in Causality", ZhiYuan Community, 2022.01<br/>-->
<!--		  <li> "Generalization Powered by Invariant Learning", Singapore Management University (SMU), 2021.11<br/>-->
<!--		  <li> "Âõ†ÊûúÊé®ÁêÜÁöÑÂ∫îÁî®‰∏éÂèëÂ±ï (‰∏≠Êñá)", AI Time, 2021.10 &nbsp<a href="https://www.bilibili.com/video/BV1qP4y1b7us?spm_id_from=333.999.0.0">[Video]</a><br/>-->
<!--		  <li> "Visual Commonsense R-CNN", National University of Singapore (NUS), 2021.03<br/>-->
<!--          </p>-->
<!--          </div>-->
<!--        </td>-->
<!--      </tr>-->
<!--</table>-->



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>üèÜ Honors & Scholarships</heading>
          <div style="line-height:25px">
          <p>
          <li> <stronghuge>Principle's Scholarship of SCUT</stronghuge>,&nbsp; 2024.<br/>
          <li> <stronghuge>First Prize of the 17th National College Student Software Contest</stronghuge>,&nbsp; 2024.<br/>
              <li> <stronghuge>Excellent Degree Dissertations of SCUT (Bachelor Degree)</stronghuge>,&nbsp; 2023. (<strong>Extended version has been accepted by TIP 2024</strong>)<br/>
		  <li> <stronghuge>Global AI Challenge for Building E&M Facilities Golden Award</stronghuge>,&nbsp; 2023.<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<!--<p></p><p></p><p></p><p></p><p></p>-->
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Project</heading>-->
<!--		  <p>-->
<!--		  <div style="text-align: center;">-->
<!--		  <img src='project/projects_icon.png'  width="600">-->
<!--		  </p>-->
<!--          </div>-->
<!--        </td>-->
<!--      </tr>-->
<!--</table>-->



<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Leadership Experience</heading>-->
<!--      </td>-->
<!--      </tr>-->
<!--      </table>-->


<!--    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >-->
<!--      <td width="15%">-->
<!--        <img src='image/lecture.jpg'  width="195" height="130">-->
<!--      </td>-->
<!--      <td valign="top" width="75%">-->
<!--        <stronghuge>Lecture Group of EE Department</stronghuge> </br>-->
<!--        <huge><em>Founder & President</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Oct. 2017 - Sep. 2018 </br>-->
<!--        <p></p>-->
<!--        <p>-->
<!--         <li> Organized academic forum, sharing sessions, Q&A meetings more than 30 times, serving over 1000 students on studying and future planing.<br/>-->
<!--          <li> The team grows to 30 people and won the Outstanding Student Organisation prize in 2018.<br/>-->
<!--          </p> -->
<!--      </td>-->
<!--    </tr>-->
<!--   </table>-->


<!--       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--    <tr onmouseout="dachuang_stop()" onmouseover="dachuang_start()" >-->
<!--      <td width="26%">-->
<!--        <div class="one">-->
<!--                <div class="two" id='dachuang_image'><img src='image/dachuang2.png'  width="195" height="130"></div>-->
<!--                <img src='image/dachuang1.png'  width="195" height="130">-->
<!--              </div>-->
<!--      <script type="text/javascript">-->
<!--                function dachuang_start() {-->
<!--                  document.getElementById('dachuang_image').style.opacity = "1";-->
<!--                }-->
<!--                function dachuang_stop() {-->
<!--                  document.getElementById('dachuang_image').style.opacity = "0";-->
<!--                }-->
<!--                dachuang_stop()-->
<!--              </script>-->
<!--      </td>-->

<!--      <td valign="top" width="75%">-->
<!--        <stronghuge>Innovative Entrepreneurship Project of UESTC</stronghuge> </br>-->
<!--        <huge><em>Team Leader</em></huge>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &bull; Sep. 2017 - Mar. 2018 </br>-->
<!--        <p></p>-->
<!--        <p>-->
<!--         <li> This project focus on the pedestrian detection in low-light condition with excellent conclusion. We combine the recent pedestrian detection models with the low-light image enhancement algorithm based on Laplace operator.<br/>-->
<!--         <li> Responsible for the code implementation and project promotion.<br/>-->
<!--          </p> -->
<!--      </td>-->
<!--    </tr>-->
<!--   </table>-->





<p></p><p></p><p></p><p></p><p></p>
<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--      <tr>-->
<!--        <td width="100%" valign="middle">-->
<!--          <heading>Personal Interests</heading>-->
<!--          <p>-->
<!--          <stronghuge>DOTA1</stronghuge>: My first and most playing PC game which accompanied me in my whole middle and high school. And I got about 1350 score  on the '11' Battle Platform Ladder Tournament. :)-->
<!--          </p>-->
<!--          <p>-->
<!--          <stronghuge>Running</stronghuge>: During my college, I offen run a long distance for the pleasure releasing. And I have participated in the Chengdu Shuangyi Marathon in 2018.-->
<!--          </p>-->
<!--      </td>-->
<!--      </tr>-->





<!--   <p></p><p></p><p></p><p></p><p></p>-->
<!--   <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=250&t=tt&d=85Rlf3OqLYVhTE6hGEcHnAsDJl6O0EsUp326ZMpLzCI"></script>-->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="left"><font size="2">
				Last updated on Mar, 2025
				<p align="middle"><font size="2">
				This website template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">[here]</a>
				</tbody></table>


</body>
</html>
